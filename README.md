# dmk

This is a simplified, automated build tool for data projects.

The idea behind dmk is to support build files that are easy to read *and*
write, and to support automating a build system for data artifacts. `dmk` was
inspired by `make`, `scons`, and `fac`.

For each command in a pipeline, you need to supply:

* A name
* The actual command to run (in the shell)
* The inputs required
* The outputs generated
* (*Optionally*) a list of intermediate files that `clean` process can delete

The file is generally named `Pipeline` or `pipeline.yaml`. The following names
will be looked for in the current directory (in order):

* Pipeline
* Pipeline.yaml
* pipeline
* pipeline.yaml
* .Pipeline.Pipeline.yaml
* .pipeline.pipeline.yaml

You may also supply a custom name with the `-f` command line flag. If the
pipeline file is in a different directory, dmk will change to that directory
before parsing the config file.

All build steps run in parallel, but each step waits until other steps build
its dependencies. A single build step executes the following steps:

1. The step is "Started"
2. If any of the required inputs are another step's outputs, then wait for a built message.
3. Check to see if *any* outputs are older than *any* of the inputs. If not, then the step is "Completed"!
4. If not done, set status to "Executing" and run the command.
5. If the command returns an error code or if *any* outputs are missing or older than *any* inputs, the step is "Failed".
6. If not done, send notification messages for each output for any waiting steps.
7. The step is now "Completed"

The outputs for a step must be unique to that step: you can't have two steps
both list `foo.data` as an output.

## Pipeline file format

The file is in YAML format where each build step is a named hash. With a few
fields specified:

* _command_ - The command to execute. It will be passed to a new bash shell, so
  it can rely on bash shell niceties (like using `~` for the home directory)
* _inputs_ - a list of inputs needed for the build. These are also the
  dependencies that must exist before the step can run. An entry can be a
  glob pattern (like `*.txt`)
* _outputs_ - a list of outputs generated by the step. These are used to decide
  if the step must and what is deleted during a clean. Glob patterns are
  **ignored** for outputs.
* _clean_ - a list of files that are deleted during a clean (in addition to the
  outputs). Nothing else is done with these files. Glob patterns are allowed.

There are a number of sample Pipeline files in the `res` subdirectory, but a
quick example would look like:

````
# You can have comments in a file
step1:                                # first step
    command: "xformxyz i{1,2,3}.txt"  # command with some shell magic
    inputs:                           # 3 inputs (read by our imaginary command)
        - i1.txt                  
        - i2.txt
        - i3.txt
    outputs:                          # 3 outputs
        - o1.txt
        - o2.txt
        - o3.txt
    clean: [a.aux, b.log]             # two extrac clean targets, specified in
                                      # an alternate syntax for YAML lists

step2:                                # second step
    command: cmd1xyz                  # note the lack of inputs - this means
    outputs:                          # the step can run immediately, but will
        - output.bin                  # ONLY run if an output is missing

depstep:                              # third/final step: it won't run until the
    command: cmd2xyz                  # previous steps are finished because their
    inputs:                           # outputs are specified in the this step's
        - o3.txt                      # inputs.
        - output.bin
    outputs:
        - combination.output
    clean:
        - need-cleaning.*             # An example of using a glob pattern
````

## Who should use this?

This is a tool for data flows and simple projects. Often these projects
involve one-time steps for getting the data used for analysis. Often the user
manually places that data in the directory after downloading it from Amazon S3
or a research server or whatever. Scripts or programs run in a pipeline
fashion: handling cleaning, transformation, analysis, model building,
figure production, presentation building, etc.

Pipelines like this are often written Python or R, partially automated with
shell scripts, and then tied together with a Makefile (or a SConstruct file if
you're an `scons` fan).

*Protip*: if you're looking for a command to handle building reports from `.tex`
files (including handling metapost and biblatex), look into `rubber`.

## What is this NOT for?

This is *not* mean to replace a real automated build tool for a software
project. As a general rule:

* If you're build Go software, use the Go tools (and optionally make)
* If you're building Java/Scala use `sbt`, `gradle`, `mvn`, `ant`, etc
* If you're building .NET, erm, I'm not sure
* There are great tools like `scons` that understand how to build lots of artifacts (including LaTeX docs)
* If you're not sure, at least understand why you wouldn't use `make`

For instance, this project (written in Go) is actually built with `make` + the
standard Go tools.

## Building

`godep` is used to manage dependencies in the vendor directory. This should be
handled transparently by the `Makefile`.

## Some helpful hints to remember

Commands are executed in a new bash shell (which also means you need bash)

File names are assumed to be relative to the Pipeline file. The current working
directory is changed to the same directory as the Pipeline file before anything
is done.

You may use globbing patterns for the inputs and clean
